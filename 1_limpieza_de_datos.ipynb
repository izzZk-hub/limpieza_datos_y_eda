{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izzZk-hub/limpieza_datos_y_eda/blob/main/1_limpieza_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUqyjKT3H_CR"
      },
      "source": [
        "# Limpieza de datos\n",
        "\n",
        "La mayoría de la gente piensa que ser científico de datos significa estar corriendo modelos avanzados de Machine Learning todo el tiempo. La realidad es muy distinta:\n",
        "\n",
        "![Tiempo de un científico de datos](https://github.com/izzZk-hub/limpieza_datos_y_eda/blob/main/img/datascientist_time.jpeg?raw=1)\n",
        "\n",
        "La gran mayoría del tiempo se va **limpiando y organizando** los datos con los que queremos trabajar.\n",
        "\n",
        "Los científicos de datos utilizamos herramientas como Pandas, Numpy, Matplotlib y Seaborn para limpiar los datos con los que queremos hacer algo.\n",
        "\n",
        "## ETL\n",
        "\n",
        "Un tipo de tarea que realizamos con gran frecuencia los científicos de datos son los **ETL**.\n",
        "\n",
        "![Proceso de ETL](https://github.com/izzZk-hub/limpieza_datos_y_eda/blob/main/img/etl.png?raw=1)\n",
        "\n",
        "ETL es un acrónimo que significa Extract, Transform, Load. Es un proceso que se utiliza para extraer datos de una fuente, transformarlos en un formato que sea adecuado para el análisis y cargarlos en una base de datos o algún otro sistema de almacenamiento.\n",
        "\n",
        "## Ejemplo práctico\n",
        "\n",
        "Los datos que usaremos para esta limpieza y nuestro siguiente análisis son datos de incidencia delictiva en nuestro país.\n",
        "\n",
        "La iniciativa de datos abiertos del gobierno de México nos proporciona datos de incidencia delictiva desde 2015 hasta la fecha. Los datos se actualizan todos los meses y se pueden descargar desde el siguiente enlace: https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva\n",
        "\n",
        "---\n",
        "\n",
        "Como podemos ver en el portal, se proporcionan los datos tanto a nivel estatal como a nivel municipal. En este caso, utilizaremos los datos a nivel estatal.\n",
        "\n",
        "Descarguemos los datos y guardemos el archivo CSV en la carpeta data con el nombre `datos_delitos.csv`.\n",
        "\n",
        "---\n",
        "\n",
        "Ahora leamos el archivo CSV y veamos cómo se ven los datos.\n",
        "\n",
        "Primero que nada, importemos pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nhKGY8_iH_CS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ai88Ie8-H_CS",
        "outputId": "d54c05d8-0b77-4f0c-9970-2d4d61f151fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/datos_delitos.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1861314408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/datos_delitos.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/datos_delitos.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('data/datos_delitos.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MMxga3KH_CS"
      },
      "source": [
        "Aquí tenemos un error muy común que suele ocurrir cuando un archivo se guarda en una computadora con cierto \"encoding\".\n",
        "\n",
        "Un encoding es una tabla que relaciona un número con un carácter. Por ejemplo, en la tabla ASCII, el número 65 corresponde a la letra \"A\".\n",
        "\n",
        "Si el archivo que estamos leyendo fue guardado con un encoding distinto al que pandas espera, nos arrojará un error.\n",
        "\n",
        "Para solucionar esto, podemos utilizar el parámetro `encoding` de la función `pd.read_csv()` y especificar el encoding correcto.\n",
        "\n",
        "¿Pero cómo sabemos con qué encoding cuenta el archivo? Si no sabemos el encoding de un archivo, primero probamos con utf-8; si falla, usamos ISO-8859-1 o latin1.\n",
        "\n",
        "La realidad es que la gran mayoría de los archivos los encontrarán en encoding utf-8 y Pandas no va a dar ningún error. Aquí estamos teniendo este problema porque la computadora que utilizan para generar este archivo uso un encoding diferente a utf-8.\n",
        "\n",
        "En México, por lo general, si un archivo no está en utf-8, lo más seguro es que esté en `ISO-8859-1` o `latin1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_oisNoOH_CS"
      },
      "source": [
        "Intentemos cargar el archivo especificando el encoding `ISO-8859-1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q8y7tiJH_CT"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/datos_delitos.csv', encoding='ISO-8859-1')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTdFztqfH_CT"
      },
      "source": [
        "Y vemos que ya podemos leer correctamente el archivo.\n",
        "\n",
        "\n",
        "¿Existe alguna forma de verificar el encoding de un archivo sin tener que estar adivinando? Sí, se puede verificar el encoding de un archivo usando librerías como chardet en Python, que detecta automáticamente el encoding más probable sin tener que adivinar.\n",
        "\n",
        "ChatGTP generó el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lk5QQWvH_CT"
      },
      "outputs": [],
      "source": [
        "import chardet\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        rawdata = f.read()\n",
        "    result = chardet.detect(rawdata)\n",
        "    return result\n",
        "\n",
        "file_path = './data/datos_delitos.csv'\n",
        "encoding_info = detect_encoding(file_path)\n",
        "print(f\"Detected encoding: {encoding_info['encoding']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIRXxhjXH_CT"
      },
      "source": [
        "Sigamos..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpiJpvSrH_CT"
      },
      "outputs": [],
      "source": [
        "df.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzo5deTVH_CT"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5QD8PSUH_CT"
      },
      "source": [
        "Bien. Están muy bien los datos. Sin embargo, tenemos un problema con el que es muy común encontrarnos.\n",
        "\n",
        "Resulta que el formato en el que está el archivo es fácil entender por seres humanos:\n",
        "\n",
        "\n",
        "```markdown\n",
        "| Año      | Entidad  | Enero    | Febrero  | Mes X    |\n",
        "|----------|----------|----------|----------|----------|\n",
        "|   ...    |   ...    |   ...    |   ...    |   ...    |\n",
        "|   ...    |   ...    |   ...    |   ...    |   ...    |\n",
        "|   ...    |   ...    |   ...    |   ...    |   ...    |\n",
        "|   ...    |   ...    |   ...    |   ...    |   ...    |\n",
        "|   ...    |   ...    |   ...    |   ...    |   ...    |\n",
        "```\n",
        "\n",
        "Vemos que tenemos los meses como encabezados. Es decir, el archivo está tratando cada mes como si fuera una variable.\n",
        "\n",
        "Nosotros como científicos de datos estamos más interesados en conjuntos de datos que no estén en este formato de \"resumen\" o \"tabla dinámica\". Para nosotros, lo ideal sería que cada mes fuera simplemente una observación más en nuestro conjunto de datos. Es decir, queremos transformar la tabla de arriba en:\n",
        "\n",
        "```markdown\n",
        "| Año      | Entidad  | Mes        |\n",
        "|----------|----------|------------|\n",
        "|   ...    |   ...    |   Enero    |\n",
        "|   ...    |   ...    |   Febrero  |\n",
        "|   ...    |   ...    |   Marzo    |\n",
        "|   ...    |   ...    |   Abril    |\n",
        "|   ...    |   ...    |   Mes X    |\n",
        "```\n",
        "\n",
        "A este tipo de formato le llamos \"formato largo de datos\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPsvBh17H_CT"
      },
      "source": [
        "Haremos las siguientes limpiezas:\n",
        "* Transformar los nombres de las columnas para que no tengan caracteres especiales y estén siempre en minúsculas\n",
        "* Convertir el dataset a un formato de datos \"largo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRT7sKp5H_CT"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvPmtpDcH_CT"
      },
      "outputs": [],
      "source": [
        "len(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcvJb1kcH_CT"
      },
      "source": [
        "### Limpiar nombres de columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUvZVumyH_CT"
      },
      "outputs": [],
      "source": [
        "def limpiar_columnas(df):\n",
        "    columnas_limpias = []\n",
        "    for col in df.columns:\n",
        "        # convertir a minusculas, reemplazar espacios por guiones bajos y eliminar caracteres especiales\n",
        "        col = col.lower().replace(\" \", \"_\").replace(\"ñ\", \"ni\").replace(\".\", \"\").replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\",\"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "        columnas_limpias.append(col)\n",
        "\n",
        "    df.columns = columnas_limpias\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQtyfaLDH_CU"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjvin-9vH_CU"
      },
      "outputs": [],
      "source": [
        "df = limpiar_columnas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td2hQ88nH_CU"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0IofUsaH_CU"
      },
      "source": [
        "Muy bien. Ahora lo que queremos hacer es quitar algunas columnas. Nos interesan nada más las siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG4Y068EH_CU"
      },
      "outputs": [],
      "source": [
        "df[['anio', 'entidad']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNuq37vHH_CU"
      },
      "outputs": [],
      "source": [
        "df = df[['anio', 'clave_ent', 'entidad', 'tipo_de_delito', 'subtipo_de_delito', 'modalidad','enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']]\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS3is5l5H_CU"
      },
      "source": [
        "### Formato largo de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxrcDaS_H_CU"
      },
      "source": [
        "Ahora, usaremos el método `melt` para convertir las columnas a observaciones.\n",
        "\n",
        "Queremos convervar las coumnas:\n",
        "* anio\n",
        "* clave_ent\n",
        "* entidad\n",
        "* tipo_de_delito\n",
        "* subtipo_de_delito\n",
        "* modalidad\n",
        "\n",
        "El resto de las columnas las vamos a juntar en una nueva columna llamada \"nombre_mes\" y sus valores los vamos a sumar en otra llamada \"frecuencia\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGnYOm-iH_CU"
      },
      "outputs": [],
      "source": [
        "print(\"Shape \", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihXucZq-H_CU"
      },
      "outputs": [],
      "source": [
        "datos_long = df.melt(id_vars=['anio', 'clave_ent', 'entidad','tipo_de_delito', 'subtipo_de_delito', 'modalidad'], var_name='nombre_mes', value_name='frecuencia')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLAy38CYH_CU"
      },
      "outputs": [],
      "source": [
        "print(\"Shape: \", datos_long.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E31Gx0wcH_CU"
      },
      "outputs": [],
      "source": [
        "datos_long.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmFStPunH_CU"
      },
      "outputs": [],
      "source": [
        "datos_long.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP_GHS-TH_CU"
      },
      "source": [
        "Supongamos que para este análisis, no nos importan los niveles subtipo de delito y modalidad. O sea, no queremos tener la distinción entre homicidios dolosos y culposos (sé que son bastante diferentes, pero simplifiquemos nuestro ejemplo).\n",
        "\n",
        "Vamos a agrupar nuestro dataframe por anio, clave_ent, entidad, tipo_de_delito y nombre_mes. Esto hará que todos los tipos de homicidios se sumen al tipo \"homicidio\" o todos los tipos de robo de vehículo (con o sin violencia) se sumen a \"robo de vehículo\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX-aPLYoH_CU"
      },
      "outputs": [],
      "source": [
        "datos_long = datos_long.groupby(['anio', 'clave_ent', 'entidad', 'tipo_de_delito', 'nombre_mes'])['frecuencia'].sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6w3XkvIH_CU"
      },
      "outputs": [],
      "source": [
        "datos_long[datos_long.tipo_de_delito == 'Robo'].sample(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgygySZzH_CV"
      },
      "source": [
        "Mostremos todos los estados y su respectiva clave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfVZFycJH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[['clave_ent', 'entidad']].drop_duplicates().sort_values('entidad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mChE9AlH_CV"
      },
      "source": [
        "Ahora Veamos todos los datos de delitos de una entidad en específico. Por ejemplo, Nuevo león"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnRKcdRLH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[datos_long['clave_ent'] == 19].sample(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhMFk-pXH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[(datos_long['clave_ent'] > 19) &  (datos_long['clave_ent'] < 24) & (datos_long['tipo_de_delito'] == 'Homicidio')].sample(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4rVvNoBH_CV"
      },
      "source": [
        "### Valores de fechas\n",
        "\n",
        "Finalmente, queremos tener una columna \"fecha\". Actualmente tenemos el año y el nombre del mes, pero no tenemos como tal una columna que tenga un tipo de dato fecha. Eso hace que filtrar por fecha sea complicado.\n",
        "\n",
        "Por ejemplo, si queremos conocer todos los homicidios de Oaxaca en enero 2024, haríamos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOVjuPGBH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[\n",
        "    (datos_long['clave_ent'] == 20) &\n",
        "    (datos_long['tipo_de_delito'] == 'Homicidio') &\n",
        "    (datos_long['anio'] == 2024) &\n",
        "    (datos_long['nombre_mes'] == 'enero')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OolscdYKH_CV"
      },
      "source": [
        "Creemos una columna de fecha.\n",
        "\n",
        "Primero tenemos que convertir el nombre de mes a un número, en donde 1 es enero, 2 febrero, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su6MBA0wH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmIIecIwH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[\"nueva_columna\"] = \"dato vacío\"\n",
        "datos_long.sample(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT2YYPMzH_CV"
      },
      "outputs": [],
      "source": [
        "# Diccionario de ayuda para convertir\n",
        "meses = {\n",
        "    \"enero\": 1,\n",
        "    \"febrero\": 2,\n",
        "    \"marzo\": 3,\n",
        "    \"abril\": 4,\n",
        "    \"mayo\": 5,\n",
        "    \"junio\": 6,\n",
        "    \"julio\": 7,\n",
        "    \"agosto\": 8,\n",
        "    \"septiembre\": 9,\n",
        "    \"octubre\": 10,\n",
        "    \"noviembre\": 11,\n",
        "    \"diciembre\": 12\n",
        "}\n",
        "\n",
        "datos_long['mes'] = datos_long['nombre_mes'].map(meses)\n",
        "datos_long.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZmEI6fGH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[\"frecuencia_mas_10\"] = datos_long[\"frecuencia\"] + 10\n",
        "datos_long.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vlT2wSpH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[\"anio_mes\"] = datos_long[\"anio\"].astype(str) + datos_long[\"mes\"].astype(str)\n",
        "datos_long.sample(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHbmYGsdH_CV"
      },
      "outputs": [],
      "source": [
        "# yyyy-mm-dd, yy-mm-dd, yymmdd, yyyy/dd/mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTuhEb-4H_CV"
      },
      "outputs": [],
      "source": [
        "# Agregamos la columna de fecha juntando el año y el mes\n",
        "datos_long['fecha'] = pd.to_datetime(datos_long['anio'].astype(str) + datos_long['mes'].astype(str), format='%Y%m')\n",
        "datos_long.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G33gsSYAH_CV"
      },
      "outputs": [],
      "source": [
        "# Eliminamos las columnas que ya no necesitamos\n",
        "datos_long = datos_long.drop(columns=['nueva_columna'])\n",
        "datos_long.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0GzV3FH_CV"
      },
      "source": [
        "Veamos los homicidios en oaxaca de enero 2024 a la fecha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJznsa7dH_CV"
      },
      "outputs": [],
      "source": [
        "datos_long[\n",
        "    (datos_long.tipo_de_delito == \"Homicidio\") &\n",
        "    (datos_long.clave_ent == 20) &\n",
        "    (datos_long.fecha >= '2024-01-01')\n",
        "].sort_values('fecha')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAi6CdS2H_CV"
      },
      "outputs": [],
      "source": [
        "datos_finales = datos_long[['anio', 'clave_ent', 'entidad', 'tipo_de_delito', 'nombre_mes', 'fecha', 'frecuencia']]\n",
        "datos_finales.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlYky3ZHH_CW"
      },
      "source": [
        "Ya que tenemos muestros datos bien estructurados, los podemos guardar en nuestra computadora. Los guardaremos con el nombre \"delitos.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSiPUClqH_CW"
      },
      "outputs": [],
      "source": [
        "datos_finales.to_csv('data/delitos.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}